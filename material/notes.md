# Summary

### Alignment of LLM

AI alignment refers to the effort to ensure that artificial intelligence (AI) systems are designed and programmed to act in accordance with human values, goals, and intentions. The goal of AI alignment is to bridge the gap between the objectives of AI systems and the objectives of their human creators or users.

There are several aspects to AI alignment, including:

1. **Value Alignment**: Ensuring that the goals and values of AI systems align with those of humans. This involves encoding human values into the AI system's objective function or reward function so that it behaves in ways that are desirable and beneficial to humans.

2. **Capability Alignment**: Ensuring that AI systems are aligned with human capabilities and limitations. This involves designing AI systems that are robust, safe, and aligned with human cognition and decision-making processes.

3. **Ethical Alignment**: Ensuring that AI systems behave ethically and responsibly. This involves ensuring that AI systems do not engage in harmful or unethical behavior, such as discrimination, bias, or manipulation.

4. **Alignment with Societal Impact**: Ensuring that AI systems have positive societal impacts and contribute to the common good. This involves considering the broader societal implications of AI technologies and designing AI systems that promote fairness, equality, and social welfare.

AI alignment is a complex and multifaceted challenge that requires interdisciplinary collaboration across fields such as computer science, ethics, psychology, and philosophy. Researchers and practitioners in AI alignment aim to develop techniques, methodologies, and best practices to address these challenges and ensure that AI technologies are developed and deployed in ways that are beneficial and aligned with human values and goals.

### Links
- [src 1](https://arxiv.org/abs/2309.15025)
- [src 2](https://openai.com/research/instruction-following)
- [src 3](https://arxiv.org/abs/2203.02155)